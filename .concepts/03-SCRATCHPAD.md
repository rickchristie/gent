# Agent scratchpad is not ReAct-specific but a universal pattern with varied implementations

The "agent scratchpad" concept has become synonymous with LangChain's agent framework, but its origins predate ReAct and its underlying pattern appears across virtually all LLM agent architectures—each using different terminology and data structures. **The scratchpad concept originated in Nye et al.'s 2021 paper "Show Your Work"** as a buffer for intermediate computation, not from the ReAct paradigm. LangChain adapted this into its `agent_scratchpad` implementation term, while academic papers use terms like "trajectory," "reasoning trace," and "working memory" for equivalent concepts.

Understanding how different agent architectures manage growing context is now considered **the #1 engineering challenge** for building effective agents. As context windows fill with tool outputs, observations, and reasoning steps, every major framework has evolved distinct strategies—from simple sliding windows to sophisticated multi-agent isolation patterns.

## The scratchpad originated from computation research, not ReAct

The foundational academic definition comes from **Nye et al. (2021)**: "We train transformers to perform multi-step computations by asking them to emit intermediate computation steps into a 'scratchpad.'" This paper demonstrated that allowing LLMs to produce arbitrary sequences of intermediate tokens dramatically improved performance on multi-step computation tasks.

**ReAct (Yao et al., 2022) never defines "agent scratchpad."** The paper instead uses "trajectories" for sequences of thought-action-observation steps and "reasoning traces" for verbal reasoning. ReAct explicitly cites Nye et al.'s scratchpad work as related but distinct, noting that "ReAct performs more than just isolated, fixed reasoning, and integrates model actions and their corresponding observations into a coherent stream of inputs."

The term "agent_scratchpad" is a **LangChain-specific implementation concept** that adapted the academic scratchpad idea for agentic loops. In LangChain, it's implemented as either a text string (for ReAct-style agents) or a message list (for function-calling agents), integrated via `MessagesPlaceholder` in prompt templates.

| Paper/Framework | State Accumulation Term | Data Structure |
|----------------|------------------------|----------------|
| Nye et al. 2021 | Scratchpad | Linear buffer |
| ReAct (Yao 2022) | Trajectory, reasoning traces | Sequential thought-action-observation |
| Tree of Thoughts (2023) | Thoughts, tree of thoughts | Tree structure |
| Graph of Thoughts (2023) | Graph Reasoning State (GRS) | Graph with vertices and edges |
| LangChain | agent_scratchpad | `List[Tuple[AgentAction, str]]` or message list |
| LlamaIndex | Context store + Memory | Typed reasoning step objects |

## How major architectures handle state accumulation

### ReAct and its direct implementations

LangChain's canonical ReAct implementation stores intermediate steps as a list of tuples: `List[Tuple[AgentAction, str]]`, where `AgentAction` contains the tool name, input, and reasoning log, paired with the observation string. Multiple formatters transform this data structure for different agent types:

```python
# Text-based formatting for ReAct
format_log_to_str(intermediate_steps) → 
"Thought: I should search...\nAction: Search\nAction Input: query\nObservation: result"

# Message-based for function calling
format_to_tool_messages(intermediate_steps) → [AIMessage, ToolMessage, ...]
```

Context growth is managed through `max_iterations` (default 15), `max_execution_time` (wall clock limit), and `trim_intermediate_steps` (custom callback for context reduction). The AgentExecutor stops when iteration limits are reached, using either "force" (hard stop) or "generate" (final LLM call) methods.

### Plan-and-Execute separates planning from execution state

Inspired by BabyAGI and Wang et al.'s Plan-and-Solve prompting (ACL 2023), Plan-and-Execute architectures maintain two distinct state components: a **task list** generated by a planning LLM and an **execution context** maintained by action agents. This separation allows the planner to operate on high-level task descriptions while executors manage detailed tool outputs, naturally preventing context explosion at the planning layer.

### Tree and Graph of Thoughts use structured state representations

**Tree of Thoughts (ToT)** maintains state as tree nodes representing "coherent language sequences that serve as intermediate steps toward solving a problem." Each node is a "thought" that gets evaluated by the LLM itself, with BFS/DFS search traversing the reasoning space. State accumulation follows tree depth rather than linear sequence.

**Graph of Thoughts (GoT)** introduces the **Graph Reasoning State (GRS)**: "a dynamic structure that maintains the state of the ongoing LLM reasoning process (the history of its thoughts and their states)." Formally modeled as tuple (G, T, E, R), where G represents all LLM thoughts with their relationships, enabling non-linear state accumulation with edges representing dependencies between thoughts.

### LlamaIndex uses typed workflow context

LlamaIndex takes a fundamentally different approach with typed objects and workflow-based state:

```python
from llama_index.core.agent.react.types import ActionReasoningStep, ObservationReasoningStep

# State stored via workflow Context
await ctx.store.set("current_reasoning", reasoning_steps)
reasoning = await ctx.store.get("current_reasoning", default=[])
```

Memory management integrates **Memory Blocks** with priority-based truncation: priority 0 blocks (static) are never truncated, while higher-priority blocks are trimmed first when token limits are exceeded. Built-in `ChatSummaryMemoryBuffer` automatically summarizes when exceeding token limits, and `token_flush_size` controls when old messages move to long-term storage.

### Multi-agent frameworks vary from stateless to multi-tiered

**AutoGPT** implements dual memory: short-term (FIFO queue of ~4000 words, last 9 messages) plus long-term (vector embeddings with top-K retrieval). For each prompt, it retrieves **K=10 relevant memories** from long-term storage, combines them with short-term context under "This reminds you of events from your past," then generates the next command. Supports five vector backends: LocalCache, Redis, Pinecone, Milvus, and Weaviate.

**BabyAGI** uses task-centric memory without separate short-term storage—a prioritized task queue plus vector database for completed task results. Three specialized agents (Execution, Task Creation, Prioritization) operate on this shared state, with semantic search informing new task execution from relevant past results.

**CrewAI** offers the most sophisticated memory architecture: short-term (ChromaDB with RAG), long-term (SQLite3 for cross-session persistence), entity memory (tracks people, places, concepts), and contextual memory (aggregates all types). Memory is crew-level by default, enabling shared knowledge bases across agents.

**AutoGen** provides a flexible Memory protocol with ListMemory (chronological), ChromaDBVectorMemory, RedisMemory, and Mem0Memory implementations. Agents are stateful between calls with built-in serialization/deserialization for session resumption.

**OpenAI Swarm** takes the opposite extreme: **explicitly stateless by design**. `client.run()` takes messages and returns messages with no saved state. Context travels through `context_variables` dict, requiring all necessary context to be explicitly passed at each handoff—maximum developer visibility at the cost of convenience.

### Coding agents developed specialized patterns for large codebases

**Claude Code** manages context through **subagent isolation**—specialized AI assistants run in their own context windows with custom system prompts, returning only relevant results to prevent context pollution. The `/compact` command triggers summarization preserving architectural decisions while compressing tool outputs. Combined with its memory tool, context editing achieves **39% improvement** on agentic search evaluations and **84% token reduction** in 100-turn evaluations.

**Cursor** pioneered **dynamic context discovery**: tool outputs written to files instead of directly to context, with agents using `tail` to check outputs and read more if needed. This achieved **46.9% token reduction** for MCP tool calls. Chat history also becomes searchable files when context fills, enabling recovery of details lost to summarization.

**Aider** uses **repository mapping** with tree-sitter parsing to create concise maps including function signatures and call graphs. A PageRank-based algorithm ranks file importance, selecting the most relevant portions that fit a configurable token budget (default 1k tokens). Users add only files needing changes while the repo map automatically pulls context from related files.

**Devin** addresses context through **pre-indexed documentation**—Devin Wiki automatically indexes repositories every few hours, generating architecture diagrams, dependency mapping, and comprehensive docs. Devin Search handles natural language codebase queries, reducing the context-gathering bottleneck.

**OpenAI's Agents SDK** uses `RunContextWrapper` for typed state shared between agents, tools, and lifecycle hooks, combined with session memory management through context trimming (drops older turns) or context summarization (compresses older turns while keeping last N verbatim).

## Four dominant context management patterns have emerged

### Sliding window approaches remove oldest content first

**Token-based windows** maintain fixed maximum token counts, removing oldest tokens when exceeded. LangChain's `ConversationBufferMemory` triggers at `max_token_limit`. This provides precise API cost control but can split mid-message.

**Turn-based windows** keep the last N message exchanges. SWE-agent's observation masking keeps the latest **10 turns**, which research shows works well for most use cases. This preserves natural conversation boundaries but varies in actual token count.

**Time-based decay** assigns relevance scores that decrease over time, with older memories weighted lower in retrieval—used in MemoryBank and similar systems.

**Hybrid approaches** trigger summarization when either limit is reached. JetBrains Research (2025) found hybrid observation masking plus LLM summarization achieved **7% cost reduction** versus pure masking and **11% versus pure summarization**.

### Summarization compresses history while preserving key information

**Recursive/progressive summarization** combines older messages with existing summaries, giving progressively older messages less influence. Claude Code's auto-compact at 95% context triggers summarization preserving architectural decisions and unresolved bugs while discarding redundant tool outputs.

**Observation masking** replaces older observations with placeholders ("details omitted for brevity") while retaining full action/reasoning history. Research shows this often **matches or beats LLM summarization** in both cost and performance because SE agent turns heavily skew toward verbose observations.

**Summary-buffer hybrids** (LangChain's `ConversationSummaryBufferMemory`) maintain raw recent messages plus progressively updated summaries of older ones. Advanced systems achieve **80-90% token reduction** while maintaining response quality.

### Memory compression uses semantic and importance-based filtering

**Semantic compression** extracts entities, resolves pronouns, and removes redundancy—converting lengthy descriptions into structured data. **Importance scoring** weights memories by relevance to current task, access frequency, and recency. Anthropic's approach clears raw tool results once tools are called deep in history, reasoning that agents don't need to re-see outputs from many turns ago.

**Active compression** has agents autonomously decide when to consolidate learnings. The Focus Agent approach summarizes trajectory into persistent "Knowledge" blocks and prunes raw history, achieving **22.7% token savings** with explicit compression prompting.

### RAG-based selective retrieval enables long-term memory

Traditional RAG treats memory as a stateless lookup table, but **agentic RAG** enables dynamic retrieval decisions, hypothetical response generation to guide retrieval, and iterative refinement. The **BEAM framework** implements three-memory architecture: episodic (long-term conversation index), working (recent turns), and scratchpad (salient facts for future use).

**Letta/MemGPT's memory blocks** provide structured storage with label, value, size limit, and descriptions—agent-editable via memory tools and individually persisted with unique IDs. This enables memories that mutate and evolve rather than static lookup.

## Alternative terminology maps to consistent underlying concepts

| Term | Definition | Common Usage |
|------|------------|--------------|
| **Scratchpad** | Temporary storage for intermediate thoughts and calculations | LangChain ReAct agents |
| **Working memory** | Short-term memory holding current context (RAM analogy) | MemGPT/Letta, cognitive science |
| **Execution trace** | Complete path from input to output including all operations | Debugging tools (Langfuse, LangSmith) |
| **Trajectory** | Sequence of (reasoning, action, observation) turns | RL literature, agent evaluation |
| **Memory stream** | Continuous record of agent experiences | Generative Agents paper (Park et al.) |
| **Knowledge block** | Structured persistent storage within context | Focus Agent, Letta |
| **Context buffer** | Store for recent exchanges with rolling window | LangChain ConversationBuffer |
| **Agent state** | Full configuration of agent at any moment | LangGraph checkpointing |

Framework-specific variations: LangChain uses "Memory" classes; LangGraph uses "State" and "Checkpoints"; OpenAI SDK uses "Trace" and "Span"; Anthropic uses "Context" and "Compaction."

## Implementation best practices from production systems

**Context engineering principles** have emerged from production experience: minimize high-signal tokens to find the smallest set maximizing desired outcome; start new sessions for new tasks to prevent context pollution; treat context as finite resource with diminishing marginal returns. Research shows **23% performance degradation** above 85% context utilization.

**Recommended approaches by scenario:**
- Short conversations (<20 turns): Simple sliding window
- Multi-session continuity: RAG + persistent memory blocks  
- Code/debugging tasks: Observation masking (preserves reasoning)
- Long documents: Progressive summarization
- Complex research: Multi-agent architecture with isolated contexts

**Cost optimization** requires monitoring token usage per session, implementing tiered priority for context elements, using smaller models for summarization, and caching repeated retrievals. Research indicates inefficient context management causes **40-60% higher API costs**.

## Conclusion

The "agent scratchpad" is not tied to ReAct but represents a universal need across all agent architectures: maintaining intermediate state during execution loops. What varies is the **data structure** (linear buffers, trees, graphs, typed objects), **management strategy** (sliding windows, summarization, compression, retrieval), and **terminology** (trajectory, working memory, execution trace).

The clear trend across all production systems is that **intelligent context selection outperforms larger context windows**. Claude Code's combination of context editing and memory tools yielding 39% improvement, Cursor's dynamic discovery achieving 46.9% token reduction, and Aider's repository maps demonstrating that structural understanding beats raw code—all point to the same insight: context engineering has become as important as model capability for effective agents.